{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# download pretrained weights\n","!gdown --folder 1nPYVv-UD2qPDStWy0hHVGPcBzDGbW2r6"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11996,"status":"ok","timestamp":1650884954383,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"FeoDS0E12IEK","outputId":"6f8d1f8a-d822-4280-d38c-7154bfc9645f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mediapipe\n","  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32.7 MB 185 kB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.0.0)\n","Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.8)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.9.1\n"]}],"source":["# install missing deps\n","!pip install mediapipe PyYAML==5.3.1"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10933,"status":"ok","timestamp":1650884965296,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"Fj9YcAnsT4B_"},"outputs":[],"source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import mediapipe as mp\n","import tensorflow as tf\n","import time\n","import copy\n","import itertools\n","import torch\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"L6pCmkJrUC9g"},"source":["## Models"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":515,"status":"ok","timestamp":1650885006240,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"uQ-q9Et14wId"},"outputs":[],"source":["# load media pipe hand detection model\n","mp_hands = mp.solutions.hands"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650885006241,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"EH43_pOL40h2"},"outputs":[],"source":["# load hand keypoints classifier (detect sign)\n","model_path = 'models/keypoint_classifier.tflite'\n","class KeyPointClassifier(object):\n","    \"\"\"\n","    Classify hand keys points into 8 gestures\n","    \n","    Note: the classification model and class has been taken and refactored from https://github.com/kinivi/tello-gesture-control\n","    \"\"\"\n","    def __init__(\n","        self,\n","        model_path=\"models/keypoint_classifier.tflite\"\n","        \n","    ):\n","        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n","        self.interpreter.allocate_tensors()\n","        self.input_details = self.interpreter.get_input_details()\n","        self.output_details = self.interpreter.get_output_details()\n","\n","    def __call__(\n","        self,\n","        frame,\n","        hand_landmarks,\n","    ):\n","        # Landmark calculation\n","        landmark_list = self._calc_landmark_list(frame, hand_landmarks)\n","\n","        # Conversion to relative coordinates / normalized coordinates\n","        pre_processed_landmark_list = self._pre_process_landmark(landmark_list)\n","\n","        input_details_tensor_index = self.input_details[0]['index']\n","        self.interpreter.set_tensor(\n","            input_details_tensor_index,\n","            np.array([pre_processed_landmark_list], dtype=np.float32))\n","        self.interpreter.invoke()\n","\n","        output_details_tensor_index = self.output_details[0]['index']\n","\n","        result = self.interpreter.get_tensor(output_details_tensor_index)\n","\n","        result_index = np.argmax(np.squeeze(result))\n","\n","        return result_index\n","    \n","    def _pre_process_landmark(self, landmark_list):\n","        temp_landmark_list = copy.deepcopy(landmark_list)\n","\n","        # Convert to relative coordinates\n","        base_x, base_y = 0, 0\n","        for index, landmark_point in enumerate(temp_landmark_list):\n","            if index == 0:\n","                base_x, base_y = landmark_point[0], landmark_point[1]\n","\n","            temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n","            temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n","\n","        # Convert to a one-dimensional list\n","        temp_landmark_list = list(\n","            itertools.chain.from_iterable(temp_landmark_list))\n","\n","        # Normalization\n","        max_value = max(list(map(abs, temp_landmark_list)))\n","\n","        def normalize_(n):\n","            return n / max_value\n","\n","        temp_landmark_list = list(map(normalize_, temp_landmark_list))\n","\n","        return temp_landmark_list\n","    \n","    def _calc_landmark_list(self, image, landmarks):\n","            image_width, image_height = image.shape[1], image.shape[0]\n","\n","            landmark_point = []\n","\n","            # Keypoint\n","            for _, landmark in enumerate(landmarks.landmark):\n","                landmark_x = min(int(landmark.x * image_width), image_width - 1)\n","                landmark_y = min(int(landmark.y * image_height), image_height - 1)\n","                # landmark_z = landmark.z\n","\n","                landmark_point.append([landmark_x, landmark_y])\n","\n","            return landmark_point\n","\n","# initialize hand keypoint classifier\n","key_point_classifier = KeyPointClassifier(model_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414,"referenced_widgets":["10562de6ea2640208b1d0db16fe2b8a0","7f3a5606097e4f7ab466a907586810b2","0033a45354834dcd8dc963974296c8f5","c9f4523bdfe74f938c12c4b04da31aa9","9f92bd51710549b5b90f9d195b2367af","f58fa6ead8ea45849a3202c48bccfe9c","277f6a57f1d84305bc2503a64f1ff7b0","42fc151aae264eccaa8870e343b123d2","e46d09a2c68f468d9a4a41183bd15f7f","5ac5521eea0e490ba6853231e02e2536","b19138aafa4d45598c7d4bc4241804bc"]},"executionInfo":{"elapsed":17590,"status":"ok","timestamp":1650885005729,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"havEtVK82i-F","outputId":"37a42aef-4edc-4f0d-b1a4-83c67e4cfa62"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n","\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n","Collecting PyYAML>=5.3.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","Installing collected packages: PyYAML\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 üöÄ 2022-4-25 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5n.pt to yolov5n.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10562de6ea2640208b1d0db16fe2b8a0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/3.87M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","Fusing layers... \n","YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n","Adding AutoShape... \n"]}],"source":["# load yolov5 nano model (for person detection)\n","yolo = torch.hub.load('ultralytics/yolov5', 'yolov5n')\n","# check if gpu is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","yolo.to(device);\n","# limit yolo model to only detect persons\n","yolo.classes = [0]"]},{"cell_type":"markdown","metadata":{"id":"L6pCmkJrUC9g"},"source":["## Helper Functions\n","Below are a few helper converting between different image data types and formats and to create the webcam video stream using javascript. "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650885006241,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"qQeLKSqm454H"},"outputs":[],"source":["def calc_bounding_rect(image, landmarks):\n","    # Calculate bounding box from hand landmarks\n","    image_width, image_height = image.shape[1], image.shape[0]\n","\n","    landmark_array = np.empty((0, 2), int)\n","\n","    for _, landmark in enumerate(landmarks.landmark):\n","        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n","        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n","\n","        landmark_point = [np.array((landmark_x, landmark_y))]\n","\n","        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n","\n","    x, y, w, h = cv2.boundingRect(landmark_array)\n","\n","    return [x, y, x + w, y + h]\n","\n","def draw_info(image, brect, hand_sign_text = \"\"):\n","    # draw bounding box of hand\n","    # Outer rectangle\n","    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n","                    (255, 0, 0), 2)\n","\n","    # Text\n","    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n","                     (255, 0, 0), -1)\n","    cv2.putText(image, hand_sign_text, (brect[0] + 5, brect[1] - 4),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n","\n","def calc_center(brect):\n","    # calculate center of hand bounding box\n","    return (brect[0] + brect[2]) / 2, (brect[1] + brect[3]) / 2\n","\n","\n","def draw_person(image, p):\n","    # draw bounding box of person\n","    cv2.rectangle(image, (int(p.xmin), int(p.ymin)), (int(p.xmax), int(p.ymax)), (0, 0, 255), 2)\n","\n","def in_bounding_box(p, bbox):\n","    # check if point is in bounding box\n","    return p[0] >= bbox[0] and p[0] <= bbox[2] and p[1] >= bbox[1] and p[1] <= bbox[3]\n","\n","def euclidian_distance(p1, p2):\n","    # calculate euclidian distance between two points\n","    return np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650885006241,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"09b_0FAnUa9y"},"outputs":[],"source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1650885171452,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"ghUlAJzKSjFT"},"outputs":[],"source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth; 640\n","      captureCanvas.height = 480; //video.videoHeight; 480\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"]},{"cell_type":"markdown","metadata":{"id":"I_xcqQZKYzAj"},"source":["## Resulting Milestone 1\n","Below is the result of what we have done for Milestone 1."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":27328,"status":"ok","timestamp":1650885200514,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"1nkSnkbkk4cC","outputId":"f0523f2d-a8d5-40a1-9c37-58cb21b78d7e"},"outputs":[{"data":{"application/javascript":"\n    var video;\n    var div = null;\n    var stream;\n    var captureCanvas;\n    var imgElement;\n    var labelElement;\n    \n    var pendingResolve = null;\n    var shutdown = false;\n    \n    function removeDom() {\n       stream.getVideoTracks()[0].stop();\n       video.remove();\n       div.remove();\n       video = null;\n       div = null;\n       stream = null;\n       imgElement = null;\n       captureCanvas = null;\n       labelElement = null;\n    }\n    \n    function onAnimationFrame() {\n      if (!shutdown) {\n        window.requestAnimationFrame(onAnimationFrame);\n      }\n      if (pendingResolve) {\n        var result = \"\";\n        if (!shutdown) {\n          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n        }\n        var lp = pendingResolve;\n        pendingResolve = null;\n        lp(result);\n      }\n    }\n    \n    async function createDom() {\n      if (div !== null) {\n        return stream;\n      }\n\n      div = document.createElement('div');\n      div.style.border = '2px solid black';\n      div.style.padding = '3px';\n      div.style.width = '100%';\n      div.style.maxWidth = '600px';\n      document.body.appendChild(div);\n      \n      const modelOut = document.createElement('div');\n      modelOut.innerHTML = \"<span>Status:</span>\";\n      labelElement = document.createElement('span');\n      labelElement.innerText = 'No data';\n      labelElement.style.fontWeight = 'bold';\n      modelOut.appendChild(labelElement);\n      div.appendChild(modelOut);\n           \n      video = document.createElement('video');\n      video.style.display = 'block';\n      video.width = div.clientWidth - 6;\n      video.setAttribute('playsinline', '');\n      video.onclick = () => { shutdown = true; };\n      stream = await navigator.mediaDevices.getUserMedia(\n          {video: { facingMode: \"environment\"}});\n      div.appendChild(video);\n\n      imgElement = document.createElement('img');\n      imgElement.style.position = 'absolute';\n      imgElement.style.zIndex = 1;\n      imgElement.onclick = () => { shutdown = true; };\n      div.appendChild(imgElement);\n      \n      const instruction = document.createElement('div');\n      instruction.innerHTML = \n          '<span style=\"color: red; font-weight: bold;\">' +\n          'When finished, click here or on the video to stop this demo</span>';\n      div.appendChild(instruction);\n      instruction.onclick = () => { shutdown = true; };\n      \n      video.srcObject = stream;\n      await video.play();\n\n      captureCanvas = document.createElement('canvas');\n      captureCanvas.width = 600; //video.videoWidth; 640\n      captureCanvas.height = 600; //video.videoHeight; 480\n      window.requestAnimationFrame(onAnimationFrame);\n      \n      return stream;\n    }\n    async function stream_frame(label, imgData) {\n      if (shutdown) {\n        removeDom();\n        shutdown = false;\n        return '';\n      }\n\n      var preCreate = Date.now();\n      stream = await createDom();\n      \n      var preShow = Date.now();\n      if (label != \"\") {\n        labelElement.innerHTML = label;\n      }\n            \n      if (imgData != \"\") {\n        var videoRect = video.getClientRects()[0];\n        imgElement.style.top = videoRect.top + \"px\";\n        imgElement.style.left = videoRect.left + \"px\";\n        imgElement.style.width = videoRect.width + \"px\";\n        imgElement.style.height = videoRect.height + \"px\";\n        imgElement.src = imgData;\n      }\n      \n      var preCapture = Date.now();\n      var result = await new Promise(function(resolve, reject) {\n        pendingResolve = resolve;\n      });\n      shutdown = false;\n      \n      return {'create': preShow - preCreate, \n              'show': preCapture - preShow, \n              'capture': Date.now() - preCapture,\n              'img': result};\n    }\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","\n","with mp_hands.Hands(\n","    model_complexity=0,\n","    min_detection_confidence=0.5,\n","    min_tracking_confidence=0.5,\n","    max_num_hands=4) as hands:\n","  while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # Process the image in a non-writable way (faster) + convert to RGB to detect hands\n","    img_copy = img.copy()\n","    img_copy.flags.writeable = False\n","    img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)\n","    results = hands.process(img_copy)\n","\n","    # detect persons in the frame\n","    yolo_pred = yolo(img)\n","    persons = yolo_pred.pandas().xyxy[0]\n","    \n","    # if hands detected\n","    if results.multi_hand_landmarks:\n","      hand_centers = []\n","      # for each hand\n","      for hand_landmarks in results.multi_hand_landmarks:\n","        # classify hand keypoints\n","        gesture_idx = key_point_classifier(img, hand_landmarks)\n","        # if sign detected (== 2, \"up\")\n","        if gesture_idx == 2:\n","          # draw bounding box\n","          brect = calc_bounding_rect(img, hand_landmarks)\n","          draw_info(bbox_array, brect, \"Sign detected\")\n","          # save center\n","          hand_center = calc_center(brect)\n","          hand_centers.append(hand_center)\n","\n","      # if the sign is detected more than once\n","      if len(hand_centers) > 1:\n","        for _, p in persons.iterrows():\n","          rel_hand_centers = []\n","          # check if at least two hands are in the person bounding box (and discard too close hand detections)\n","          for hand_center in hand_centers:\n","            dup = False\n","            for rhc in rel_hand_centers:\n","              if euclidian_distance(hand_center, rhc) < 50:\n","                dup = True\n","                break\n","            if in_bounding_box(hand_center, [p.xmin, p.ymin, p.xmax, p.ymax]) and not dup:\n","              rel_hand_centers.append(hand_center)\n","          if len(rel_hand_centers) > 1:\n","            draw_person(bbox_array, p)\n","            break\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650885050333,"user":{"displayName":"Sushen Jilla","userId":"01204390629975492576"},"user_tz":-120},"id":"3e4NrNs4GTtW"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Milestone 1.ipynb","provenance":[{"file_id":"1QnC7lV7oVFk5OZCm75fqbLAfD9qBy9bw","timestamp":1650728036958}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0033a45354834dcd8dc963974296c8f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_42fc151aae264eccaa8870e343b123d2","max":4062133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e46d09a2c68f468d9a4a41183bd15f7f","value":4062133}},"10562de6ea2640208b1d0db16fe2b8a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f3a5606097e4f7ab466a907586810b2","IPY_MODEL_0033a45354834dcd8dc963974296c8f5","IPY_MODEL_c9f4523bdfe74f938c12c4b04da31aa9"],"layout":"IPY_MODEL_9f92bd51710549b5b90f9d195b2367af"}},"277f6a57f1d84305bc2503a64f1ff7b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42fc151aae264eccaa8870e343b123d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ac5521eea0e490ba6853231e02e2536":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f3a5606097e4f7ab466a907586810b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f58fa6ead8ea45849a3202c48bccfe9c","placeholder":"‚Äã","style":"IPY_MODEL_277f6a57f1d84305bc2503a64f1ff7b0","value":"100%"}},"9f92bd51710549b5b90f9d195b2367af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b19138aafa4d45598c7d4bc4241804bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9f4523bdfe74f938c12c4b04da31aa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac5521eea0e490ba6853231e02e2536","placeholder":"‚Äã","style":"IPY_MODEL_b19138aafa4d45598c7d4bc4241804bc","value":" 3.87M/3.87M [00:00&lt;00:00, 44.5MB/s]"}},"e46d09a2c68f468d9a4a41183bd15f7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f58fa6ead8ea45849a3202c48bccfe9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
